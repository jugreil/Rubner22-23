{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a List with 20 3Dimensional Points between -1 and 1\n",
    "import random\n",
    "data = []\n",
    "points = []\n",
    "for r in range(2):\n",
    "    for i in range(20):\n",
    "        points.append([random.uniform(-1,1),random.uniform(-1,1),random.uniform(-1,1)])\n",
    "    data.append(points.copy())\n",
    "    points.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from IPython import display\n",
    "from tensorflow import keras\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 20, 3)\n",
      "(5000, 5)\n",
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(20, 3), dtype=tf.float64, name=None), TensorSpec(shape=(5,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#opening and reading the file schere.json, papier.json and stein.json\n",
    "#and saving the data in the variable schere, papier, stein, echse, spock\n",
    "schere = []\n",
    "papier = []\n",
    "stein = []\n",
    "echse = []\n",
    "spock = []\n",
    "\n",
    "with open('schere.json', \"r\") as f:\n",
    "    schere = json.load(f)\n",
    "with open('papier.json', \"r\") as f:\n",
    "    papier = json.load(f)\n",
    "with open('stein.json', \"r\") as f:\n",
    "    stein = json.load(f)\n",
    "with open('echse.json', \"r\") as f:\n",
    "    echse = json.load(f)\n",
    "with open('spock.json', \"r\") as f:\n",
    "    spock = json.load(f)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in schere:\n",
    "    data.append(i)\n",
    "for i in papier:\n",
    "    data.append(i)\n",
    "for i in stein:\n",
    "    data.append(i)\n",
    "for i in echse:\n",
    "    data.append(i)\n",
    "for i in spock:\n",
    "    data.append(i)\n",
    "\n",
    "\n",
    "\n",
    "data = np.array(data)\n",
    "data = data.reshape(5000, 20, 3)\n",
    "print(data.shape)\n",
    "\n",
    "schere_label = np.array([0, 0, 1,0,0])\n",
    "papier_label = np.array([0, 1, 0,0,0])\n",
    "stein_label = np.array([1, 0, 0,0,0])\n",
    "echse_label = np.array([0, 0, 0,0,1])\n",
    "spock_label = np.array([0, 0, 0,1,0])\n",
    "\n",
    "list = [\"stein\", \"papier\", \"schere\", \"spock\", \"echse\"]\n",
    "labels = np.concatenate([\n",
    "    np.tile(schere_label, (1000, 1)),\n",
    "    np.tile(papier_label, (1000, 1)),\n",
    "    np.tile(stein_label, (1000, 1)),\n",
    "    np.tile(echse_label, (1000, 1)),\n",
    "    np.tile(spock_label, (1000, 1))\n",
    "])\n",
    "\n",
    "labels = labels.reshape(5000, 5)\n",
    "print(labels.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "print(dataset)\n",
    "# Split the dataset into train and validation.\n",
    "train_dataset = dataset.take(4200)\n",
    "val_dataset = dataset.skip(4200)\n",
    "\n",
    "# Shuffle and batch the datasets.\n",
    "BATCH_SIZE = 4\n",
    "SHUFFLE_BUFFER_SIZE = 200\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 20, 3)             12        \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 20, 16)            64        \n",
      "                                                                 \n",
      " flatten_31 (Flatten)        (None, 320)               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 32)                10272     \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,513\n",
      "Trainable params: 10,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(3, activation='relu', input_shape=(20,3)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='softmax'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1050/1050 [==============================] - 6s 6ms/step - loss: 0.1324 - accuracy: 0.9521 - val_loss: 1.3842 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1050/1050 [==============================] - 7s 7ms/step - loss: 0.1204 - accuracy: 0.9593 - val_loss: 0.6357 - val_accuracy: 0.7050\n",
      "Epoch 3/10\n",
      "1050/1050 [==============================] - 8s 7ms/step - loss: 0.1305 - accuracy: 0.9514 - val_loss: 0.5568 - val_accuracy: 0.8863\n",
      "Epoch 4/10\n",
      "1050/1050 [==============================] - 8s 7ms/step - loss: 0.1383 - accuracy: 0.9500 - val_loss: 1.1942 - val_accuracy: 0.0012\n",
      "Epoch 5/10\n",
      "1050/1050 [==============================] - 7s 7ms/step - loss: 0.1203 - accuracy: 0.9598 - val_loss: 0.5142 - val_accuracy: 0.9175\n",
      "Epoch 6/10\n",
      "1050/1050 [==============================] - 6s 6ms/step - loss: 0.1285 - accuracy: 0.9524 - val_loss: 0.5249 - val_accuracy: 0.8263\n",
      "Epoch 7/10\n",
      "1050/1050 [==============================] - 7s 6ms/step - loss: 0.1388 - accuracy: 0.9505 - val_loss: 0.6225 - val_accuracy: 0.6988\n",
      "Epoch 8/10\n",
      "1050/1050 [==============================] - 7s 6ms/step - loss: 0.1272 - accuracy: 0.9552 - val_loss: 0.6830 - val_accuracy: 0.7588\n",
      "Epoch 9/10\n",
      "1050/1050 [==============================] - 7s 7ms/step - loss: 0.1286 - accuracy: 0.9526 - val_loss: 0.7905 - val_accuracy: 0.6700\n",
      "Epoch 10/10\n",
      "1050/1050 [==============================] - 7s 7ms/step - loss: 0.1048 - accuracy: 0.9707 - val_loss: 0.5099 - val_accuracy: 0.8313\n"
     ]
    }
   ],
   "source": [
    "#training the model with the dataset\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10,batch_size=4)\n",
    "\n",
    "#saving the model\n",
    "model.save('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[2.3076928e-04 3.8353911e-01 2.1455227e-05 5.9974569e-01 1.6462903e-02]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[9.9972194e-01 8.0163873e-06 2.3072393e-05 2.1025880e-06 2.4488108e-04]]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "with open('schere_test.json', \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test = np.array(test)\n",
    "test = test.reshape(1, 20, 3)\n",
    "prediction = model.predict(test)\n",
    "print(prediction.argmax())\n",
    "test = []\n",
    "with open('papier_test.json', \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test = np.array(test)\n",
    "test = test.reshape(1, 20, 3)\n",
    "prediction = model.predict(test)\n",
    "print(prediction)\n",
    "test = []\n",
    "with open('stein_test.json', \"r\") as f:\n",
    "    test = json.load(f)\n",
    "test = np.array(test)\n",
    "test = test.reshape(1, 20, 3)\n",
    "prediction = model.predict(test)\n",
    "print(prediction)\n",
    "#0 = stein"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
